{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56194f0",
   "metadata": {},
   "source": [
    "# Extração de Imagens de Cicatrizes de Incêndios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c3a350",
   "metadata": {},
   "source": [
    "Este notebook implementa o fluxo de **seleção e extração de pares de imagens Sentinel-2 (pré e pós-incêndio)** a partir de focos de queimadas registrados pelo INPE, com foco no Cerrado dos estados do **Maranhão** e **Tocantins**. O resultado final é um arquivo `.csv` com as coordenadas e metadados das cenas que serão usadas nas etapas seguintes do projeto (cálculo de índices espectrais, segmentação fuzzy/quantum etc.).\n",
    "\n",
    "De forma resumida, o fluxo é:\n",
    "\n",
    "1. **Carregamento de bibliotecas e conexão ao STAC**\n",
    "   - Importa bibliotecas de geoprocessamento, análise de dados e imagens (`geopandas`, `pandas`, `numpy`, `matplotlib`, `rasterio`, `shapely`, `folium`, `pystac_client`, entre outras).\n",
    "   - Define o endpoint do **STAC do BDC/INPE** (`https://data.inpe.br/bdc/stac/v1/`) e abre o catálogo para consulta da coleção Sentinel-2 (`S2_L2A-1`).\n",
    "\n",
    "2. **Leitura e filtragem dos focos de incêndio**\n",
    "   - Lê um arquivo `.csv` com os focos de incêndio (BDQueimadas).\n",
    "   - Aplica filtros para selecionar apenas os focos mais relevantes:\n",
    "     - Estados: **MARANHÃO / TOCANTINS**;\n",
    "     - `RiscoFogo` ≥ 0,85;\n",
    "     - `FRP` ≥ 70.\n",
    "   - Converte a coluna de data/hora (`DataHora`) para o tipo `datetime`, facilitando operações temporais.\n",
    "\n",
    "3. **Transformação dos focos em áreas de interesse (AOIs)**\n",
    "   - Converte o `DataFrame` filtrado em um **GeoDataFrame** (pontos a partir de latitude/longitude).\n",
    "   - Reprojeta os focos para um sistema métrico (`EPSG:3857`) e **agrupa por dia**.\n",
    "   - Para cada dia:\n",
    "     - Cria um **buffer de 5 km** em torno dos focos e dissolve as geometrias para obter áreas contínuas queimadas/potenciais.\n",
    "     - Converte essas áreas de volta para coordenadas geográficas (`EPSG:4326`).\n",
    "     - Extrai as **bounding boxes** de cada área e registra, para cada AOI:\n",
    "       - data,\n",
    "       - bounding box,\n",
    "       - número de focos (`n_focos`),\n",
    "       - mediana do FRP,\n",
    "       - média do risco de fogo.\n",
    "\n",
    "4. **Priorização das AOIs mais promissoras**\n",
    "   - Organiza a lista de AOIs em um `DataFrame` (`bboxes_df`).\n",
    "   - Garante os tipos numéricos e trata valores ausentes.\n",
    "   - Calcula um **score de relevância** (com base em FRP, risco e número de focos) e faz um ranking.\n",
    "   - Para cada dia, seleciona as **AOIs prioritárias** (por exemplo, o **TOP-K** por data), gerando o conjunto `priorizadas`.\n",
    "\n",
    "5. **Consulta ao STAC para obter cenas pré e pós-incêndio**\n",
    "   - Para cada AOI priorizada, consulta o catálogo STAC do BDC:\n",
    "     - Coleção: `S2_L2A-1`;\n",
    "     - Janela temporal em torno da data do foco (por exemplo, alguns dias **antes** e **depois** do incêndio);\n",
    "     - Limite máximo de cobertura de nuvens (`cloud_cover`).\n",
    "   - Identifica, para cada AOI/data, um par de cenas:\n",
    "     - **Imagem pré-incêndio** (anterior à data do foco);\n",
    "     - **Imagem pós-incêndio** (posterior à data do foco).\n",
    "   - Armazena os pares em uma lista `pairs`, incluindo:\n",
    "     - data do cluster (`cluster_date`),\n",
    "     - chave/região,\n",
    "     - bounding box,\n",
    "     - FRP mediano e risco médio,\n",
    "     - IDs das cenas pré e pós,\n",
    "     - datas de aquisição,\n",
    "     - cobertura de nuvens.\n",
    "\n",
    "6. **Construção do DataFrame de pares e remoção de duplicatas**\n",
    "   - Converte `pairs` em um `DataFrame` (`pairs_df`), ordenado por data e região.\n",
    "   - Remove pares duplicados de (`pre_id`, `post_id`), evitando repetição de cenas em múltiplos alertas muito próximos.\n",
    "\n",
    "7. **Visualização rápida das cenas selecionadas**\n",
    "   - Implementa funções auxiliares para:\n",
    "     - Buscar **quicklooks** ou composições RGB diretamente do STAC;\n",
    "     - Baixar e exibir as imagens pré e pós-incêndio lado a lado.\n",
    "   - A função `show_pair(i)` permite inspecionar visualmente o par de imagens correspondente à linha `i` de `pairs_df`, facilitando a escolha manual das melhores cenas.\n",
    "\n",
    "8. **Seleção final e exportação dos pares escolhidos**\n",
    "   - Após inspecionar visualmente os pares, o usuário informa uma lista de índices de interesse (por exemplo, `[n_1, n_2, n_3, ...]`).\n",
    "   - O notebook seleciona essas linhas em `pairs_df`, gera o `DataFrame` **`df_final_images`** e salva:\n",
    "     - `Imagens_e_indices/coordenadas_e_informacoes_cenas_usadas.csv`\n",
    "   - Esse arquivo concentra **as coordenadas, datas e metadados dos pares de cenas Sentinel-2 pré/pós-incêndio** que serão usados nas etapas seguintes de análise e segmentação.\n",
    "\n",
    "---\n",
    "\n",
    "Em resumo, este notebook faz a ponte entre os **focos de incêndio pontuais** (BDQueimadas) e um **conjunto curado de cenas Sentinel-2 pré/pós-incêndio**, automatizando boa parte da seleção espacial e temporal e deixando apenas a curadoria final (visual) a cargo do usuário.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7713bb",
   "metadata": {},
   "source": [
    "## Bibliotecas usadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66863d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from skimage.morphology import (\n",
    "    binary_closing,\n",
    "    binary_erosion,\n",
    "    binary_dilation,\n",
    "    remove_small_objects,\n",
    "    remove_small_holes,\n",
    "    disk\n",
    ")\n",
    "from shapely.geometry import box\n",
    "import folium\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import rasterio\n",
    "from rasterio import Affine\n",
    "from rasterio.crs import CRS\n",
    "#import rasterio.transform\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.warp import Resampling, reproject, transform\n",
    "\n",
    "import pystac_client\n",
    "pystac_client.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb8c19c",
   "metadata": {},
   "source": [
    "### Definindo catálogo e API (STAC client e data.inpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f9b999",
   "metadata": {},
   "source": [
    "Para isso utilizaremos o serviço Spatio Temporal Asset Catalog (STAC) por meio de um client na linguagem de programação Python.\n",
    "\n",
    "O endereço do serviço STAC do BDC é https://data.inpe.br/bdc/stac/v1/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "servico  = \"https://data.inpe.br/bdc/stac/v1/\"   \n",
    "catalogo  = pystac_client.Client.open(servico)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fa7737",
   "metadata": {},
   "source": [
    "## Extração Pontual de Imagens:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4e723e",
   "metadata": {},
   "source": [
    "### Importando Arquivo de Focos de Incêndio, aplicando filtros e transformando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6c437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"focos_incêndios_cerrado/focos_qmd_inpe_2024-08-01_2024-10-31_08.347472.csv\") # Lendo arquivo que possui dados dos focos de incêndio\n",
    "\n",
    "# Aplique os filtros\n",
    "filtro = (\n",
    "    (df[\"Estado\"].isin([\"MARANHÃO\",\"MARANHAO\", \"TOCANTINS\"])) &\n",
    "    (df[\"RiscoFogo\"] >= 0.85) &\n",
    "    (df[\"FRP\"] >= 70)\n",
    ")\n",
    "\n",
    "# Selecione apenas as linhas que atendem aos filtros\n",
    "df_filtrado = df[filtro]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3020f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir as primeiras linhas do resultado\n",
    "df_filtrado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76824fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado['DataHora'] = pd.to_datetime(df_filtrado['DataHora'], utc=True, errors='coerce') \n",
    "# Converte a coluna 'DataHora' para datetime(Mais facil manipular datas e realizar operações)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a0506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "gpd.GeoDataFrame(...)\n",
    "Cria um GeoDataFrame (objeto do geopandas) a partir do DataFrame focos_filt.\n",
    "Diferente de um pandas.DataFrame comum, ele entende geometrias (pontos, polígonos, linhas) e permite fazer operações espaciais (interseção, buffer, dissolver, reprojetar etc.).\n",
    "\n",
    "geometry=gpd.points_from_xy(...)\n",
    "Converte as colunas de Longitude e Latitude em uma coluna de geometria do tipo Point. Cada foco de incêndio vira um ponto no espaço.\n",
    "\n",
    "crs=\"EPSG:4326\"\n",
    "Define o sistema de referência espacial (CRS) como WGS84 (EPSG:4326), que é latitude/longitude em graus decimais — o padrão usado pelo INPE, STAC, Sentinel, etc.\n",
    "'''\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df_filtrado,\n",
    "    geometry=gpd.points_from_xy(df_filtrado['Longitude'], df_filtrado['Latitude']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edd1b3f",
   "metadata": {},
   "source": [
    "### Transformando focos em áreas de interesse (AOIs) para o STAC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793d3e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reprojetar para métrico antes de fazer buffer\n",
    "focos_m = gdf.to_crs(epsg=3857)  # projeta para Web Mercator (metros)\n",
    "focos_m['date'] = focos_m['DataHora'].dt.date\n",
    "\n",
    "bboxes = []\n",
    "for dt, sub in focos_m.groupby('date'):\n",
    "    # Buffer de 5 km (5000 m)\n",
    "    area = sub.buffer(5000).unary_union  # dissolve\n",
    "    # Converter de volta para lat/long\n",
    "    area4326 = gpd.GeoSeries([area], crs=3857).to_crs(4326).iloc[0]\n",
    "    # Gerar bounding boxes (multi‑polígonos separados)\n",
    "    geoms = list(area4326.geoms) if hasattr(area4326, 'geoms') else [area4326]\n",
    "    for geom in geoms:\n",
    "        minx, miny, maxx, maxy = geom.bounds\n",
    "        bboxes.append({\n",
    "            'date': pd.to_datetime(dt),\n",
    "            'bbox': [minx, miny, maxx, maxy],\n",
    "            'n_focos': len(sub),\n",
    "            'frp_median': sub['FRP'].median(),\n",
    "            'risco_mean': sub['RiscoFogo'].mean(),\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a03c39",
   "metadata": {},
   "source": [
    "### Elencando as áreas de interesse mais promissoras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_df = pd.DataFrame(bboxes).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86bdba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9727ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### CÓDIDO DUPLICADO PARA TESTE\n",
    "\n",
    "\n",
    "# garantir tipos, (tranformando as datas em datetime e os números em float, o que permite operações numéricas e comparações)\n",
    "bboxes_df[\"date\"] = pd.to_datetime(bboxes_df[\"date\"])\n",
    "for c in [\"frp_median\", \"risco_mean\", \"n_focos\"]:\n",
    "    bboxes_df[c] = pd.to_numeric(bboxes_df[c], errors=\"coerce\")\n",
    "#_____________________________________________________________\n",
    "\n",
    "\n",
    "\n",
    "# 2) (opcional) tratar NaN (se houver, o que não deve acontecer)\n",
    "bboxes_df[\"frp_median\"] = bboxes_df[\"frp_median\"].fillna(0)\n",
    "bboxes_df[\"risco_mean\"] = bboxes_df[\"risco_mean\"].fillna(0)\n",
    "bboxes_df[\"n_focos\"] = bboxes_df[\"n_focos\"].fillna(0)\n",
    "#_____________________________________________________________\n",
    "\n",
    "\n",
    "\n",
    "# 3) (opcional) criar um score combinado para ordenar melhor\n",
    "# normaliza colunas (min-max por TODO o período; se preferir, normalize por dia)\n",
    "def minmax(x):\n",
    "    x = x.astype(float)\n",
    "    if x.max() == x.min():\n",
    "        return np.zeros_like(x, dtype=float)\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "bboxes_df[\"frp_n\"]   = minmax(bboxes_df[\"frp_median\"])\n",
    "bboxes_df[\"risco_n\"] = minmax(bboxes_df[\"risco_mean\"])\n",
    "bboxes_df[\"nfocos_n\"]= minmax(bboxes_df[\"n_focos\"])\n",
    "#_____________________________________________________________\n",
    "\n",
    "\n",
    "# peso ajustável; aqui dou mais peso a FRP e n_focos\n",
    "w_frp, w_risco, w_nfocos = 0.5, 0.2, 0.3\n",
    "bboxes_df[\"score\"] = (w_frp*bboxes_df[\"frp_n\"] +\n",
    "                      w_risco*bboxes_df[\"risco_n\"] +\n",
    "                      w_nfocos*bboxes_df[\"nfocos_n\"])\n",
    "#_____________________________________________________________\n",
    "\n",
    "\n",
    "\n",
    "# 4) ordenar global (útil pra inspecionar)\n",
    "bboxes_df = bboxes_df.sort_values([\"date\", \"score\"], ascending=[True, False]).reset_index(drop=True)\n",
    "#_____________________________________________________________\n",
    "\n",
    "\n",
    "\n",
    "# 5) RANQUEAR por dia (top-K por data)\n",
    "TOPK = 1  # ajuste como quiser\n",
    "topk_por_dia = (\n",
    "    bboxes_df\n",
    "    .groupby(\"date\", group_keys=False)\n",
    "    .apply(lambda df: df.nlargest(TOPK, \"score\"))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "#_____________________________________________________________\n",
    "\n",
    "\n",
    "priorizadas = topk_por_dia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71fbac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "priorizadas.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265d18ae",
   "metadata": {},
   "source": [
    "### Consultando o STAC do INPE com as AOIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8cb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Parâmetros de teste\n",
    "# =========================\n",
    "SERVICO_STAC = \"https://data.inpe.br/bdc/stac/v1/\"\n",
    "COLECAO = \"S2_L2A-1\"\n",
    "cloud_lt = 15                 # mais restrito p/ teste\n",
    "pre_days = 6\n",
    "post_days = 6\n",
    "DATE_WIN = 20           \n",
    "MAX_CLUSTERS = 100           \n",
    "MAX_ITEMS_PER_CLUSTER = 100  # limitar nº de itens analisados por cluster\n",
    "\n",
    "catalogo = pystac_client.Client.open(SERVICO_STAC)\n",
    "cloud_filter = {\"eo:cloud_cover\": {\"lt\": cloud_lt}}\n",
    "\n",
    "# -------------------------\n",
    "# Utils para \"mesma região\"\n",
    "# -------------------------\n",
    "def mgrs_or_bbox_key(item):\n",
    "    \"\"\"Tenta pegar o tile MGRS do item; se não existir,\n",
    "    volta uma 'assinatura' via bbox arredondada.\"\"\"\n",
    "    props = item.properties or {}\n",
    "\n",
    "    # chaves possíveis (varia entre catálogos)\n",
    "    candidates = [\n",
    "        \"s2:tile_id\", \"s2:mgrs_tile\",\n",
    "        \"mgrs:tile\", \"mgrs:code\", \"grid:code\",\n",
    "        # combinação UTM/latitude_band/grid_square às vezes aparece\n",
    "        (\"sentinel:utm_zone\", \"sentinel:latitude_band\", \"sentinel:grid_square\"),\n",
    "    ]\n",
    "\n",
    "    for c in candidates:\n",
    "        if isinstance(c, tuple):\n",
    "            a, b, d = (props.get(c[0]), props.get(c[1]), props.get(c[2]))\n",
    "            if a and b and d:\n",
    "                return f\"MGRS:{a}{b}{d}\"\n",
    "        else:\n",
    "            if props.get(c):\n",
    "                return f\"MGRS:{props[c]}\"\n",
    "\n",
    "    # fallback: chave pelo bbox (arredondado)\n",
    "    minx, miny, maxx, maxy = item.bbox\n",
    "    r = lambda v: round(float(v), 4)\n",
    "    return f\"BBOX:{r(minx)},{r(miny)},{r(maxx)},{r(maxy)}\"\n",
    "\n",
    "\n",
    "# ---------- Utils de tempo (padrão: UTC tz-aware) ----------\n",
    "def to_utc_ts(x):\n",
    "    \"\"\"\n",
    "    Converte qualquer objeto de data (str/datetime/Timestamp) para pandas.Timestamp\n",
    "    tz-aware em UTC. Retorna None se não der pra converter.\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    ts = pd.to_datetime(x, utc=True, errors=\"coerce\")\n",
    "    if ts is None:\n",
    "        return None\n",
    "    # se por algum motivo vier sem tz (raro c/ utc=True), localiza em UTC\n",
    "    if getattr(ts, \"tz\", None) is None:\n",
    "        ts = ts.tz_localize(\"UTC\")\n",
    "    return ts\n",
    "\n",
    "def item_datetime_utc(item):\n",
    "    \"\"\"\n",
    "    Pega a melhor data/hora de um item STAC e devolve tz-aware em UTC.\n",
    "    Prioriza 'datetime'; se não houver, tenta 'start_datetime'/'end_datetime'\n",
    "    e, por último, item.datetime.\n",
    "    \"\"\"\n",
    "    p = getattr(item, \"properties\", {}) or {}\n",
    "    if p.get(\"datetime\"):\n",
    "        return to_utc_ts(p[\"datetime\"])\n",
    "    # alguns catálogos usam intervalo:\n",
    "    if p.get(\"start_datetime\"):\n",
    "        return to_utc_ts(p[\"start_datetime\"])\n",
    "    if p.get(\"end_datetime\"):\n",
    "        return to_utc_ts(p[\"end_datetime\"])\n",
    "    # fallback\n",
    "    if getattr(item, \"datetime\", None) is not None:\n",
    "        return to_utc_ts(item.datetime)\n",
    "    return None\n",
    "\n",
    "# substitui a sua antiga `to_date`\n",
    "def to_date(item):\n",
    "    return item_datetime_utc(item)\n",
    "\n",
    "# -------------------------\n",
    "# Busca + pareamento\n",
    "# -------------------------\n",
    "pairs = []  # lista final com pares antes/depois\n",
    "\n",
    "print(f\"Buscando pares p/ até {MAX_CLUSTERS} clusters...\")\n",
    "\n",
    "for _, row in priorizadas.head(MAX_CLUSTERS).iterrows():\n",
    "    print(f\"Cluster {row['date'].date()} (score={row['score']:.3f})...\")\n",
    "    # row[\"date\"] possivelmente vem como date (sem hora/tz). Coloque em UTC:\n",
    "    foco_date = to_utc_ts(row[\"date\"])              # vira 00:00:00Z daquele dia\n",
    "    # caso queira centralizar no meio do dia, você pode somar horas:\n",
    "    # foco_date = foco_date + pd.Timedelta(hours=12)\n",
    "\n",
    "    # janelinha curta p/ teste (aqui pode ser DATE_WIN tz-naive,\n",
    "    # pois vamos usar só para compor strings de busca do STAC)\n",
    "    date_range = f\"{(foco_date.tz_convert('UTC') - timedelta(days=DATE_WIN)).date()}/\" \\\n",
    "                 f\"{(foco_date.tz_convert('UTC') + timedelta(days=DATE_WIN)).date()}\"\n",
    "\n",
    "    search = catalogo.search(\n",
    "        bbox=row[\"bbox\"],\n",
    "        collections=[COLECAO],\n",
    "        query=cloud_filter,\n",
    "        datetime=date_range\n",
    "    )\n",
    "    items = list(search.get_items())\n",
    "    print(f\"  → {len(items)} itens encontrados.\")\n",
    "    if not items:\n",
    "        print(\"⚠️ Nenhum item encontrado.\")\n",
    "        continue\n",
    "\n",
    "    items = items[:MAX_ITEMS_PER_CLUSTER]\n",
    "    \n",
    "    ############################################################### Aqui tudo funcionando #######################################################\n",
    "\n",
    "    # agrupar por região\n",
    "    region_groups = {}\n",
    "    for it in items:\n",
    "        key = mgrs_or_bbox_key(it)\n",
    "        region_groups.setdefault(key, []).append(it)\n",
    "\n",
    "    # cortes tz-aware\n",
    "    pre_cut  = foco_date - timedelta(days=pre_days)\n",
    "    post_cut = foco_date + timedelta(days=post_days)\n",
    "\n",
    "    for region_key, its in region_groups.items():\n",
    "        # ordenar por datetime em UTC\n",
    "        its_sorted = sorted(its, key=to_date)\n",
    "\n",
    "        # candidatos pré/pós (tudo tz-aware)\n",
    "        pre_candidates  = [it for it in its_sorted if to_date(it) is not None and to_date(it) <= pre_cut]\n",
    "        post_candidates = [it for it in its_sorted if to_date(it) is not None and to_date(it) >= post_cut]\n",
    "\n",
    "        pre_item  = pre_candidates[-1] if pre_candidates else None\n",
    "        post_item = post_candidates[0]  if post_candidates else None\n",
    "\n",
    "        if pre_item and post_item:\n",
    "            pairs.append({\n",
    "                    \"cluster_date\": foco_date.tz_convert(\"UTC\").date(),  # ✅ nome certo e date()\n",
    "                    \"region_key\":   region_key,\n",
    "                    \"bbox\":         row[\"bbox\"],\n",
    "                    \"frp_median\":   row.get(\"frp_median\"),\n",
    "                    \"risco_mean\":   row.get(\"risco_mean\"),\n",
    "                    \"pre_id\":       pre_item.id,\n",
    "                    \"pre_datetime\": to_date(pre_item),   # tz-aware\n",
    "                    \"pre_cloud\":    pre_item.properties.get(\"eo:cloud_cover\"),\n",
    "                    \"post_id\":      post_item.id,\n",
    "                    \"post_datetime\":to_date(post_item),  # tz-aware\n",
    "                    \"post_cloud\":   post_item.properties.get(\"eo:cloud_cover\"),\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcdf13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df = pd.DataFrame(pairs)\n",
    "pairs_df = pairs_df.sort_values([\"cluster_date\",\"region_key\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a69eb8",
   "metadata": {},
   "source": [
    "#### Pegando imagens não repetidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3423423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df = pairs_df.drop_duplicates(subset=[\"pre_id\", \"post_id\"]) # remover duplicatas exatas de imagens(as vezes existem multiplos alertas de incêndio para o mesmo foco)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3b6e1",
   "metadata": {},
   "source": [
    "#### Tentando vizualizar pares de imagens encontrados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a5b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pystac_client\n",
    "import pandas as pd\n",
    "\n",
    "SERVICO_STAC = \"https://data.inpe.br/bdc/stac/v1/\"\n",
    "COLECAO = \"S2_L2A-1\"\n",
    "\n",
    "catalogo = pystac_client.Client.open(SERVICO_STAC)\n",
    "\n",
    "def pick_quicklook_url(item):\n",
    "    # tenta encontrar uma pré-visualização pronta\n",
    "    for key in [\"thumbnail\",\"visual\",\"overview\",\"quicklook\"]:\n",
    "        if key in item.assets:\n",
    "            return item.assets[key].href\n",
    "    return None  # sem quicklook disponível\n",
    "\n",
    "def fetch_image(url):\n",
    "    r = requests.get(url, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return Image.open(io.BytesIO(r.content))\n",
    "\n",
    "def show_pair(idx):\n",
    "    row = pairs_df.iloc[idx]\n",
    "\n",
    "    def fetch_item(item_id):\n",
    "        search = catalogo.search(\n",
    "            collections=[COLECAO],\n",
    "            ids=[item_id]\n",
    "        )\n",
    "        itens = list(search.get_items())\n",
    "        return itens[0] if itens else None\n",
    "\n",
    "    pre_item  = fetch_item(row[\"pre_id\"])\n",
    "    post_item = fetch_item(row[\"post_id\"])\n",
    "\n",
    "    pre_url  = pick_quicklook_url(pre_item)\n",
    "    post_url = pick_quicklook_url(post_item)\n",
    "\n",
    "    # mostra o índice real do DataFrame junto do índice passado\n",
    "    print(f\"[{idx}] (pairs_df index={row.name}) cluster_date={row['cluster_date']}, region={row['region_key']}\")\n",
    "    print(\"pre:\", row[\"pre_id\"], row[\"pre_datetime\"], \"cloud:\", row[\"pre_cloud\"], \"→\", pre_url)\n",
    "    print(\"post:\", row[\"post_id\"], row[\"post_datetime\"], \"cloud:\", row[\"post_cloud\"], \"→\", post_url)\n",
    "\n",
    "    if pre_url and post_url:\n",
    "        im_pre  = fetch_image(pre_url)\n",
    "        im_post = fetch_image(post_url)\n",
    "\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.subplot(1,2,1); plt.title(f\"PRE (row {row.name})\");  plt.axis(\"off\"); plt.imshow(im_pre)\n",
    "        plt.subplot(1,2,2); plt.title(f\"POST (row {row.name})\"); plt.axis(\"off\"); plt.imshow(im_post)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Quicklook não disponível para algum dos itens; tente o fluxo RGB (B04,B03,B02).\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b20d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 3):\n",
    "    show_pair(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968cd3b5",
   "metadata": {},
   "source": [
    "## Gerando Df_final com relações das imagens usadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82399ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# exemplo:\n",
    "lista1 = [n_1,n_2,n_3]  # substitua n_1, n_2, n_3 pelos índices desejados com nas imagens de interesse acima (Esses itens correpondem aos itens visualizados com show_pair)\n",
    "\n",
    "# seleciona as linhas desejadas de cada dataset\n",
    "subset1 = pairs_df.iloc[lista1]\n",
    "\n",
    "# concatena preservando as colunas\n",
    "df_final_images = pd.concat([subset1], ignore_index=True)\n",
    "\n",
    "import os\n",
    "if not os.path.exists(\"Imagens_e_indices\"):\n",
    "    os.makedirs(\"Imagens_e_indices\")\n",
    "\n",
    "df_final_images.to_csv(\"Imagens_e_indices/coordenadas_e_informacoes_cenas_usadas.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
